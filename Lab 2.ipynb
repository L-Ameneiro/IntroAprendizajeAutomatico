{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Armado de un esquema de aprendizaje autom√°tico\n",
    "\n",
    "En el laboratorio final se espera que puedan poner en pr√°ctica los conocimientos adquiridos en el curso, trabajando con un conjunto de datos de clasificaci√≥n.\n",
    "\n",
    "El objetivo es que se introduzcan en el desarrollo de un esquema para hacer tareas de aprendizaje autom√°tico: selecci√≥n de un modelo, ajuste de hiperpar√°metros y evaluaci√≥n.\n",
    "\n",
    "El conjunto de datos a utilizar est√° en `./data/loan_data.csv`. Si abren el archivo ver√°n que al principio (las l√≠neas que empiezan con `#`) describen el conjunto de datos y sus atributos (incluyendo el atributo de etiqueta o clase).\n",
    "\n",
    "Se espera que hagan uso de las herramientas vistas en el curso. Se espera que hagan uso especialmente de las herramientas brindadas por `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Agregar las librer√≠as que hagan falta\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos y divisi√≥n en entrenamiento y evaluaci√≥n\n",
    "\n",
    "La celda siguiente se encarga de la carga de datos (haciendo uso de pandas). Estos ser√°n los que se trabajar√°n en el resto del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"./data/loan_data.csv\", comment=\"#\")\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/DiploDatos/IntroduccionAprendizajeAutomatico/master/data/loan_data.csv\", comment=\"#\")\n",
    "\n",
    "\n",
    "# Divisi√≥n entre instancias y etiquetas\n",
    "X, y = dataset.iloc[:, 1:], dataset.TARGET\n",
    "\n",
    "# divisi√≥n entre entrenamiento y evaluaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Documentaci√≥n:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET     0\n",
      "LOAN       0\n",
      "MORTDUE    0\n",
      "VALUE      0\n",
      "YOJ        0\n",
      "DEROG      0\n",
      "DELINQ     0\n",
      "CLAGE      0\n",
      "NINQ       0\n",
      "CLNO       0\n",
      "DEBTINC    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# valores nulos\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que no hay valores nulos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n original: TARGET\n",
      "0    0.833333\n",
      "1    0.166667\n",
      "Name: proportion, dtype: float64\n",
      "Train: TARGET\n",
      "0    0.836143\n",
      "1    0.163857\n",
      "Name: proportion, dtype: float64\n",
      "Test: TARGET\n",
      "0    0.822102\n",
      "1    0.177898\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribuci√≥n de variable target en subconjuntos de train y test\n",
    "print(\"Distribuci√≥n original:\", y.value_counts(normalize=True))\n",
    "print(\"Train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Test:\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que los subconjuntos de train y test presentan distribuciones semejantes de la variable TARGET. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Descripci√≥n de los Datos y la Tarea\n",
    "\n",
    "Responder las siguientes preguntas:\n",
    "\n",
    "1. ¬øDe qu√© se trata el conjunto de datos?\n",
    "\n",
    "The Home Equity dataset (HMEQ) contains baseline and loan performance\n",
    "information for 5,960 recent home equity loans.\n",
    "\n",
    "\n",
    "2. ¬øCu√°l es la variable objetivo que hay que predecir? ¬øQu√© significado tiene?\n",
    "\n",
    "The target (BAD) is a binary\n",
    "variable indicating whether an applicant eventually defaulted or was\n",
    "seriously delinquent. \n",
    "This adverse outcome occurred in 1,189 cases (20%). \n",
    "\n",
    "3. ¬øQu√© informaci√≥n (atributos) hay disponible para hacer la predicci√≥n?\n",
    "\n",
    "For\n",
    "each applicant, 12 input variables were recorded. \n",
    "\n",
    "LOAN    Amount of the loan request\n",
    "MORTDUE Amount due on existing mortgage\n",
    "VALUE   Value of current property\n",
    "YOJ     Years at present job\n",
    "DEROG   Number of major derogatory reports\n",
    "DELINQ  Number of delinquent credit lines\n",
    "CLAGE   Age of oldest trade line in months\n",
    "NINQ    Number of recent credit lines\n",
    "CLNO    Number of credit lines\n",
    "DEBTINC Debt-to-income ratio\n",
    "\n",
    "| Variable           | Tipo              | Descripci√≥n                                                                                                                                                |\n",
    "| ------------------ | ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **TARGET** (o BAD) | Binaria (0/1)     | **Variable objetivo.** Indica si el solicitante cay√≥ en default:<br>üü¢ `0` = Pr√©stamo pagado correctamente.<br>üî¥ `1` = Incumplimiento o morosidad severa. |\n",
    "| **LOAN**           | Num√©rica continua | Monto solicitado en el pr√©stamo.                                                                                                                    |\n",
    "| **MORTDUE**        | Num√©rica continua | Saldo actual de la hipoteca existente sobre la propiedad.                                                                                                  |\n",
    "| **VALUE**          | Num√©rica continua | Valor estimado de la propiedad del solicitante.                                                                                                            |\n",
    "| **YOJ**            | Num√©rica discreta | A√±os de antig√ºedad del solicitante en su empleo actual.                                                                                                    |\n",
    "| **DEROG**          | Num√©rica discreta | Cantidad de reportes importantes de cr√©dito negativo (ej. bancarrotas, juicios).                                                                           |\n",
    "| **DELINQ**         | Num√©rica discreta | N√∫mero de l√≠neas de cr√©dito con moras registradas.                                                                                                         |\n",
    "| **CLAGE**          | Num√©rica continua | Antig√ºedad de la l√≠nea de cr√©dito m√°s antigua, en meses. Es un indicador de experiencia crediticia.                                                        |\n",
    "| **NINQ**           | Num√©rica discreta | N√∫mero de l√≠neas de cr√©dito abiertas recientemente (indicador de actividad reciente).                                                               |\n",
    "| **CLNO**           | Num√©rica discreta | N√∫mero total de l√≠neas de cr√©dito abiertas (tarjetas, pr√©stamos, etc.).                                                                                    |\n",
    "| **DEBTINC**        | Num√©rica continua | Relaci√≥n deuda-ingresos (%). Se calcula como: *(total de deudas mensuales / ingreso mensual)*. Un valor alto puede indicar mayor riesgo crediticio.        |\n",
    "\n",
    "\n",
    "4. ¬øQu√© atributos imagina ud. que son los m√°s determinantes para la predicci√≥n?\n",
    "\n",
    "- **DEBTINC**: Es un indicador directo de la capacidad de pago del solicitante. Una persona con mucha deuda en relaci√≥n a sus ingresos tiene m√°s riesgo de incumplir.\n",
    "\n",
    "Cuanto m√°s alto sea este ratio, m√°s probable es que haya dificultad para cumplir con nuevas obligaciones.\n",
    "\n",
    "- **DEROG**: La presencia de eventos crediticios graves (como bancarrota o juicios) es un fuerte indicador de riesgo hist√≥rico.\n",
    "\n",
    "- **DELINQ**: Aunque menos grave que DEROG, refleja incumplimientos recientes o frecuentes, lo cual es √∫til para predecir problemas futuros.\n",
    "\n",
    "**No hace falta escribir c√≥digo para responder estas preguntas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Predicci√≥n con Modelos Lineales\n",
    "\n",
    "En este ejercicio se entrenar√°n modelos lineales de clasificaci√≥n para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deber√°n utilizar la clase SGDClassifier de scikit-learn.\n",
    "\n",
    "Documentaci√≥n:\n",
    "- https://scikit-learn.org/stable/modules/sgd.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos un escaleo de los datos ya que SGDClasiffier usa el m√©todo de descenso por gradiente, el cual es sensible a la escala. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las columnas son num√©ricas en este dataset\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1: SGDClassifier con hiperpar√°metros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador SGDClassifier usando los valores por omisi√≥n de scikit-learn para todos los par√°metros. √önicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci√≥n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "SGDClassifier(\n",
      "    loss=\u001b[33m'hinge'\u001b[39m,\n",
      "    *,\n",
      "    penalty=\u001b[33m'l2'\u001b[39m,\n",
      "    alpha=\u001b[32m0.0001\u001b[39m,\n",
      "    l1_ratio=\u001b[32m0.15\u001b[39m,\n",
      "    fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    max_iter=\u001b[32m1000\u001b[39m,\n",
      "    tol=\u001b[32m0.001\u001b[39m,\n",
      "    shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    verbose=\u001b[32m0\u001b[39m,\n",
      "    epsilon=\u001b[32m0.1\u001b[39m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    learning_rate=\u001b[33m'optimal'\u001b[39m,\n",
      "    eta0=\u001b[32m0.0\u001b[39m,\n",
      "    power_t=\u001b[32m0.5\u001b[39m,\n",
      "    early_stopping=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    validation_fraction=\u001b[32m0.1\u001b[39m,\n",
      "    n_iter_no_change=\u001b[32m5\u001b[39m,\n",
      "    class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    average=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m SGDClassifier(BaseSGDClassifier):\n",
      "    \u001b[33m\"\"\"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\u001b[39m\n",
      "\n",
      "\u001b[33m    This estimator implements regularized linear models with stochastic\u001b[39m\n",
      "\u001b[33m    gradient descent (SGD) learning: the gradient of the loss is estimated\u001b[39m\n",
      "\u001b[33m    each sample at a time and the model is updated along the way with a\u001b[39m\n",
      "\u001b[33m    decreasing strength schedule (aka learning rate). SGD allows minibatch\u001b[39m\n",
      "\u001b[33m    (online/out-of-core) learning via the `partial_fit` method.\u001b[39m\n",
      "\u001b[33m    For best results using the default learning rate schedule, the data should\u001b[39m\n",
      "\u001b[33m    have zero mean and unit variance.\u001b[39m\n",
      "\n",
      "\u001b[33m    This implementation works with data represented as dense or sparse arrays\u001b[39m\n",
      "\u001b[33m    of floating point values for the features. The model it fits can be\u001b[39m\n",
      "\u001b[33m    controlled with the loss parameter; by default, it fits a linear support\u001b[39m\n",
      "\u001b[33m    vector machine (SVM).\u001b[39m\n",
      "\n",
      "\u001b[33m    The regularizer is a penalty added to the loss function that shrinks model\u001b[39m\n",
      "\u001b[33m    parameters towards the zero vector using either the squared euclidean norm\u001b[39m\n",
      "\u001b[33m    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\u001b[39m\n",
      "\u001b[33m    parameter update crosses the 0.0 value because of the regularizer, the\u001b[39m\n",
      "\u001b[33m    update is truncated to 0.0 to allow for learning sparse models and achieve\u001b[39m\n",
      "\u001b[33m    online feature selection.\u001b[39m\n",
      "\n",
      "\u001b[33m    Read more in the :ref:`User Guide <sgd>`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',\\\u001b[39m\n",
      "\u001b[33m        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',\\\u001b[39m\n",
      "\u001b[33m        'squared_epsilon_insensitive'}, default='hinge'\u001b[39m\n",
      "\u001b[33m        The loss function to be used.\u001b[39m\n",
      "\n",
      "\u001b[33m        - 'hinge' gives a linear SVM.\u001b[39m\n",
      "\u001b[33m        - 'log_loss' gives logistic regression, a probabilistic classifier.\u001b[39m\n",
      "\u001b[33m        - 'modified_huber' is another smooth loss that brings tolerance to\u001b[39m\n",
      "\u001b[33m          outliers as well as probability estimates.\u001b[39m\n",
      "\u001b[33m        - 'squared_hinge' is like hinge but is quadratically penalized.\u001b[39m\n",
      "\u001b[33m        - 'perceptron' is the linear loss used by the perceptron algorithm.\u001b[39m\n",
      "\u001b[33m        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\u001b[39m\n",
      "\u001b[33m          'squared_epsilon_insensitive' are designed for regression but can be useful\u001b[39m\n",
      "\u001b[33m          in classification as well; see\u001b[39m\n",
      "\u001b[33m          :class:`~sklearn.linear_model.SGDRegressor` for a description.\u001b[39m\n",
      "\n",
      "\u001b[33m        More details about the losses formulas can be found in the :ref:`User Guide\u001b[39m\n",
      "\u001b[33m        <sgd_mathematical_formulation>` and you can find a visualisation of the loss\u001b[39m\n",
      "\u001b[33m        functions in\u001b[39m\n",
      "\u001b[33m        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.\u001b[39m\n",
      "\n",
      "\u001b[33m    penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\u001b[39m\n",
      "\u001b[33m        The penalty (aka regularization term) to be used. Defaults to 'l2'\u001b[39m\n",
      "\u001b[33m        which is the standard regularizer for linear SVM models. 'l1' and\u001b[39m\n",
      "\u001b[33m        'elasticnet' might bring sparsity to the model (feature selection)\u001b[39m\n",
      "\u001b[33m        not achievable with 'l2'. No penalty is added when set to `None`.\u001b[39m\n",
      "\n",
      "\u001b[33m        You can see a visualisation of the penalties in\u001b[39m\n",
      "\u001b[33m        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\u001b[39m\n",
      "\n",
      "\u001b[33m    alpha : float, default=0.0001\u001b[39m\n",
      "\u001b[33m        Constant that multiplies the regularization term. The higher the\u001b[39m\n",
      "\u001b[33m        value, the stronger the regularization. Also used to compute the\u001b[39m\n",
      "\u001b[33m        learning rate when `learning_rate` is set to 'optimal'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    l1_ratio : float, default=0.15\u001b[39m\n",
      "\u001b[33m        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\u001b[39m\n",
      "\u001b[33m        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\u001b[39m\n",
      "\u001b[33m        Only used if `penalty` is 'elasticnet'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, 1.0]` or can be `None` if\u001b[39m\n",
      "\u001b[33m        `penalty` is not `elasticnet`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 1.7\u001b[39m\n",
      "\u001b[33m            `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\u001b[39m\n",
      "\n",
      "\u001b[33m    fit_intercept : bool, default=True\u001b[39m\n",
      "\u001b[33m        Whether the intercept should be estimated or not. If False, the\u001b[39m\n",
      "\u001b[33m        data is assumed to be already centered.\u001b[39m\n",
      "\n",
      "\u001b[33m    max_iter : int, default=1000\u001b[39m\n",
      "\u001b[33m        The maximum number of passes over the training data (aka epochs).\u001b[39m\n",
      "\u001b[33m        It only impacts the behavior in the ``fit`` method, and not the\u001b[39m\n",
      "\u001b[33m        :meth:`partial_fit` method.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[1, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.19\u001b[39m\n",
      "\n",
      "\u001b[33m    tol : float or None, default=1e-3\u001b[39m\n",
      "\u001b[33m        The stopping criterion. If it is not None, training will stop\u001b[39m\n",
      "\u001b[33m        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\u001b[39m\n",
      "\u001b[33m        epochs.\u001b[39m\n",
      "\u001b[33m        Convergence is checked against the training loss or the\u001b[39m\n",
      "\u001b[33m        validation loss depending on the `early_stopping` parameter.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.19\u001b[39m\n",
      "\n",
      "\u001b[33m    shuffle : bool, default=True\u001b[39m\n",
      "\u001b[33m        Whether or not the training data should be shuffled after each epoch.\u001b[39m\n",
      "\n",
      "\u001b[33m    verbose : int, default=0\u001b[39m\n",
      "\u001b[33m        The verbosity level.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    epsilon : float, default=0.1\u001b[39m\n",
      "\u001b[33m        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\u001b[39m\n",
      "\u001b[33m        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\u001b[39m\n",
      "\u001b[33m        For 'huber', determines the threshold at which it becomes less\u001b[39m\n",
      "\u001b[33m        important to get the prediction exactly right.\u001b[39m\n",
      "\u001b[33m        For epsilon-insensitive, any differences between the current prediction\u001b[39m\n",
      "\u001b[33m        and the correct label are ignored if they are less than this threshold.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_jobs : int, default=None\u001b[39m\n",
      "\u001b[33m        The number of CPUs to use to do the OVA (One Versus All, for\u001b[39m\n",
      "\u001b[33m        multi-class problems) computation.\u001b[39m\n",
      "\u001b[33m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39m\n",
      "\u001b[33m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39m\n",
      "\u001b[33m        for more details.\u001b[39m\n",
      "\n",
      "\u001b[33m    random_state : int, RandomState instance, default=None\u001b[39m\n",
      "\u001b[33m        Used for shuffling the data, when ``shuffle`` is set to ``True``.\u001b[39m\n",
      "\u001b[33m        Pass an int for reproducible output across multiple function calls.\u001b[39m\n",
      "\u001b[33m        See :term:`Glossary <random_state>`.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[0, 2**32 - 1]`.\u001b[39m\n",
      "\n",
      "\u001b[33m    learning_rate : str, default='optimal'\u001b[39m\n",
      "\u001b[33m        The learning rate schedule:\u001b[39m\n",
      "\n",
      "\u001b[33m        - 'constant': `eta = eta0`\u001b[39m\n",
      "\u001b[33m        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\u001b[39m\n",
      "\u001b[33m          where `t0` is chosen by a heuristic proposed by Leon Bottou.\u001b[39m\n",
      "\u001b[33m        - 'invscaling': `eta = eta0 / pow(t, power_t)`\u001b[39m\n",
      "\u001b[33m        - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\u001b[39m\n",
      "\u001b[33m          Each time n_iter_no_change consecutive epochs fail to decrease the\u001b[39m\n",
      "\u001b[33m          training loss by tol or fail to increase validation score by tol if\u001b[39m\n",
      "\u001b[33m          `early_stopping` is `True`, the current learning rate is divided by 5.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'adaptive' option.\u001b[39m\n",
      "\n",
      "\u001b[33m    eta0 : float, default=0.0\u001b[39m\n",
      "\u001b[33m        The initial learning rate for the 'constant', 'invscaling' or\u001b[39m\n",
      "\u001b[33m        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\u001b[39m\n",
      "\u001b[33m        the default schedule 'optimal'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    power_t : float, default=0.5\u001b[39m\n",
      "\u001b[33m        The exponent for inverse scaling learning rate.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `(-inf, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    early_stopping : bool, default=False\u001b[39m\n",
      "\u001b[33m        Whether to use early stopping to terminate training when validation\u001b[39m\n",
      "\u001b[33m        score is not improving. If set to `True`, it will automatically set aside\u001b[39m\n",
      "\u001b[33m        a stratified fraction of training data as validation and terminate\u001b[39m\n",
      "\u001b[33m        training when validation score returned by the `score` method is not\u001b[39m\n",
      "\u001b[33m        improving by at least tol for n_iter_no_change consecutive epochs.\u001b[39m\n",
      "\n",
      "\u001b[33m        See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an\u001b[39m\n",
      "\u001b[33m        example of the effects of early stopping.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'early_stopping' option\u001b[39m\n",
      "\n",
      "\u001b[33m    validation_fraction : float, default=0.1\u001b[39m\n",
      "\u001b[33m        The proportion of training data to set aside as validation set for\u001b[39m\n",
      "\u001b[33m        early stopping. Must be between 0 and 1.\u001b[39m\n",
      "\u001b[33m        Only used if `early_stopping` is True.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `(0.0, 1.0)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'validation_fraction' option\u001b[39m\n",
      "\n",
      "\u001b[33m    n_iter_no_change : int, default=5\u001b[39m\n",
      "\u001b[33m        Number of iterations with no improvement to wait before stopping\u001b[39m\n",
      "\u001b[33m        fitting.\u001b[39m\n",
      "\u001b[33m        Convergence is checked against the training loss or the\u001b[39m\n",
      "\u001b[33m        validation loss depending on the `early_stopping` parameter.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[1, max_iter)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'n_iter_no_change' option\u001b[39m\n",
      "\n",
      "\u001b[33m    class_weight : dict, {class_label: weight} or \"balanced\", default=None\u001b[39m\n",
      "\u001b[33m        Preset for the class_weight fit parameter.\u001b[39m\n",
      "\n",
      "\u001b[33m        Weights associated with classes. If not given, all classes\u001b[39m\n",
      "\u001b[33m        are supposed to have weight one.\u001b[39m\n",
      "\n",
      "\u001b[33m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[39m\n",
      "\u001b[33m        weights inversely proportional to class frequencies in the input data\u001b[39m\n",
      "\u001b[33m        as ``n_samples / (n_classes * np.bincount(y))``.\u001b[39m\n",
      "\n",
      "\u001b[33m    warm_start : bool, default=False\u001b[39m\n",
      "\u001b[33m        When set to True, reuse the solution of the previous call to fit as\u001b[39m\n",
      "\u001b[33m        initialization, otherwise, just erase the previous solution.\u001b[39m\n",
      "\u001b[33m        See :term:`the Glossary <warm_start>`.\u001b[39m\n",
      "\n",
      "\u001b[33m        Repeatedly calling fit or partial_fit when warm_start is True can\u001b[39m\n",
      "\u001b[33m        result in a different solution than when calling fit a single time\u001b[39m\n",
      "\u001b[33m        because of the way the data is shuffled.\u001b[39m\n",
      "\u001b[33m        If a dynamic learning rate is used, the learning rate is adapted\u001b[39m\n",
      "\u001b[33m        depending on the number of samples already seen. Calling ``fit`` resets\u001b[39m\n",
      "\u001b[33m        this counter, while ``partial_fit`` will result in increasing the\u001b[39m\n",
      "\u001b[33m        existing counter.\u001b[39m\n",
      "\n",
      "\u001b[33m    average : bool or int, default=False\u001b[39m\n",
      "\u001b[33m        When set to `True`, computes the averaged SGD weights across all\u001b[39m\n",
      "\u001b[33m        updates and stores the result in the ``coef_`` attribute. If set to\u001b[39m\n",
      "\u001b[33m        an int greater than 1, averaging will begin once the total number of\u001b[39m\n",
      "\u001b[33m        samples seen reaches `average`. So ``average=10`` will begin\u001b[39m\n",
      "\u001b[33m        averaging after seeing 10 samples.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[1, n_samples]`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else \\\u001b[39m\n",
      "\u001b[33m            (n_classes, n_features)\u001b[39m\n",
      "\u001b[33m        Weights assigned to the features.\u001b[39m\n",
      "\n",
      "\u001b[33m    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\u001b[39m\n",
      "\u001b[33m        Constants in decision function.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_iter_ : int\u001b[39m\n",
      "\u001b[33m        The actual number of iterations before reaching the stopping criterion.\u001b[39m\n",
      "\u001b[33m        For multiclass fits, it is the maximum over every binary fit.\u001b[39m\n",
      "\n",
      "\u001b[33m    classes_ : array of shape (n_classes,)\u001b[39m\n",
      "\n",
      "\u001b[33m    t_ : int\u001b[39m\n",
      "\u001b[33m        Number of weight updates performed during training.\u001b[39m\n",
      "\u001b[33m        Same as ``(n_iter_ * n_samples + 1)``.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_features_in_ : int\u001b[39m\n",
      "\u001b[33m        Number of features seen during :term:`fit`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.24\u001b[39m\n",
      "\n",
      "\u001b[33m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39m\n",
      "\u001b[33m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[39m\n",
      "\u001b[33m        has feature names that are all strings.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.0\u001b[39m\n",
      "\n",
      "\u001b[33m    See Also\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    sklearn.svm.LinearSVC : Linear support vector classification.\u001b[39m\n",
      "\u001b[33m    LogisticRegression : Logistic regression.\u001b[39m\n",
      "\u001b[33m    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\u001b[39m\n",
      "\u001b[33m        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\u001b[39m\n",
      "\u001b[33m        penalty=None)``.\u001b[39m\n",
      "\n",
      "\u001b[33m    Examples\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    >>> import numpy as np\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.linear_model import SGDClassifier\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.preprocessing import StandardScaler\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.pipeline import make_pipeline\u001b[39m\n",
      "\u001b[33m    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\u001b[39m\n",
      "\u001b[33m    >>> Y = np.array([1, 1, 2, 2])\u001b[39m\n",
      "\u001b[33m    >>> # Always scale the input. The most convenient way is to use a pipeline.\u001b[39m\n",
      "\u001b[33m    >>> clf = make_pipeline(StandardScaler(),\u001b[39m\n",
      "\u001b[33m    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\u001b[39m\n",
      "\u001b[33m    >>> clf.fit(X, Y)\u001b[39m\n",
      "\u001b[33m    Pipeline(steps=[('standardscaler', StandardScaler()),\u001b[39m\n",
      "\u001b[33m                    ('sgdclassifier', SGDClassifier())])\u001b[39m\n",
      "\u001b[33m    >>> print(clf.predict([[-0.8, -1]]))\u001b[39m\n",
      "\u001b[33m    [1]\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    _parameter_constraints: dict = {\n",
      "        **BaseSGDClassifier._parameter_constraints,\n",
      "        \u001b[33m\"penalty\"\u001b[39m: [StrOptions({\u001b[33m\"l2\"\u001b[39m, \u001b[33m\"l1\"\u001b[39m, \u001b[33m\"elasticnet\"\u001b[39m}), \u001b[38;5;28;01mNone\u001b[39;00m],\n",
      "        \u001b[33m\"alpha\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "        \u001b[33m\"l1_ratio\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, closed=\u001b[33m\"both\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m],\n",
      "        \u001b[33m\"power_t\"\u001b[39m: [Interval(Real, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"neither\"\u001b[39m)],\n",
      "        \u001b[33m\"epsilon\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "        \u001b[33m\"learning_rate\"\u001b[39m: [\n",
      "            StrOptions({\u001b[33m\"constant\"\u001b[39m, \u001b[33m\"optimal\"\u001b[39m, \u001b[33m\"invscaling\"\u001b[39m, \u001b[33m\"adaptive\"\u001b[39m}),\n",
      "            Hidden(StrOptions({\u001b[33m\"pa1\"\u001b[39m, \u001b[33m\"pa2\"\u001b[39m})),\n",
      "        ],\n",
      "        \u001b[33m\"eta0\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "    }\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        loss=\u001b[33m\"hinge\"\u001b[39m,\n",
      "        *,\n",
      "        penalty=\u001b[33m\"l2\"\u001b[39m,\n",
      "        alpha=\u001b[32m0.0001\u001b[39m,\n",
      "        l1_ratio=\u001b[32m0.15\u001b[39m,\n",
      "        fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        max_iter=\u001b[32m1000\u001b[39m,\n",
      "        tol=\u001b[32m1e-3\u001b[39m,\n",
      "        shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        verbose=\u001b[32m0\u001b[39m,\n",
      "        epsilon=DEFAULT_EPSILON,\n",
      "        n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        learning_rate=\u001b[33m\"optimal\"\u001b[39m,\n",
      "        eta0=\u001b[32m0.0\u001b[39m,\n",
      "        power_t=\u001b[32m0.5\u001b[39m,\n",
      "        early_stopping=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        validation_fraction=\u001b[32m0.1\u001b[39m,\n",
      "        n_iter_no_change=\u001b[32m5\u001b[39m,\n",
      "        class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        average=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            loss=loss,\n",
      "            penalty=penalty,\n",
      "            alpha=alpha,\n",
      "            l1_ratio=l1_ratio,\n",
      "            fit_intercept=fit_intercept,\n",
      "            max_iter=max_iter,\n",
      "            tol=tol,\n",
      "            shuffle=shuffle,\n",
      "            verbose=verbose,\n",
      "            epsilon=epsilon,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            learning_rate=learning_rate,\n",
      "            eta0=eta0,\n",
      "            power_t=power_t,\n",
      "            early_stopping=early_stopping,\n",
      "            validation_fraction=validation_fraction,\n",
      "            n_iter_no_change=n_iter_no_change,\n",
      "            class_weight=class_weight,\n",
      "            warm_start=warm_start,\n",
      "            average=average,\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _check_proba(self):\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.loss \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m (\u001b[33m\"log_loss\"\u001b[39m, \u001b[33m\"modified_huber\"\u001b[39m):\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
      "                \u001b[33m\"probability estimates are not available for loss=%r\"\u001b[39m % self.loss\n",
      "            )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "    @available_if(_check_proba)\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m predict_proba(self, X):\n",
      "        \u001b[33m\"\"\"Probability estimates.\u001b[39m\n",
      "\n",
      "\u001b[33m        This method is only available for log loss and modified Huber loss.\u001b[39m\n",
      "\n",
      "\u001b[33m        Multiclass probability estimates are derived from binary (one-vs.-rest)\u001b[39m\n",
      "\u001b[33m        estimates by simple normalization, as recommended by Zadrozny and\u001b[39m\n",
      "\u001b[33m        Elkan.\u001b[39m\n",
      "\n",
      "\u001b[33m        Binary probability estimates for loss=\"modified_huber\" are given by\u001b[39m\n",
      "\u001b[33m        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\u001b[39m\n",
      "\u001b[33m        it is necessary to perform proper probability calibration by wrapping\u001b[39m\n",
      "\u001b[33m        the classifier with\u001b[39m\n",
      "\u001b[33m        :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\u001b[39m\n",
      "\n",
      "\u001b[33m        Parameters\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        X : {array-like, sparse matrix}, shape (n_samples, n_features)\u001b[39m\n",
      "\u001b[33m            Input data for prediction.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns\u001b[39m\n",
      "\u001b[33m        -------\u001b[39m\n",
      "\u001b[33m        ndarray of shape (n_samples, n_classes)\u001b[39m\n",
      "\u001b[33m            Returns the probability of the sample for each class in the model,\u001b[39m\n",
      "\u001b[33m            where classes are ordered as they are in `self.classes_`.\u001b[39m\n",
      "\n",
      "\u001b[33m        References\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        Zadrozny and Elkan, \"Transforming classifier scores into multiclass\u001b[39m\n",
      "\u001b[33m        probability estimates\", SIGKDD'02,\u001b[39m\n",
      "\u001b[33m        https://dl.acm.org/doi/pdf/10.1145/775047.775151\u001b[39m\n",
      "\n",
      "\u001b[33m        The justification for the formula in the loss=\"modified_huber\"\u001b[39m\n",
      "\u001b[33m        case is in the appendix B in:\u001b[39m\n",
      "\u001b[33m        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.loss == \u001b[33m\"log_loss\"\u001b[39m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._predict_proba_lr(X)\n",
      "\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.loss == \u001b[33m\"modified_huber\"\u001b[39m:\n",
      "            binary = len(self.classes_) == \u001b[32m2\u001b[39m\n",
      "            scores = self.decision_function(X)\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "                prob2 = np.ones((scores.shape[\u001b[32m0\u001b[39m], \u001b[32m2\u001b[39m))\n",
      "                prob = prob2[:, \u001b[32m1\u001b[39m]\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                prob = scores\n",
      "\n",
      "            np.clip(scores, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, prob)\n",
      "            prob += \u001b[32m1.0\u001b[39m\n",
      "            prob /= \u001b[32m2.0\u001b[39m\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "                prob2[:, \u001b[32m0\u001b[39m] -= prob\n",
      "                prob = prob2\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# the above might assign zero to all classes, which doesn't\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# normalize neatly; work around this to produce uniform\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# probabilities\u001b[39;00m\n",
      "                prob_sum = prob.sum(axis=\u001b[32m1\u001b[39m)\n",
      "                all_zero = prob_sum == \u001b[32m0\u001b[39m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m np.any(all_zero):\n",
      "                    prob[all_zero, :] = \u001b[32m1\u001b[39m\n",
      "                    prob_sum[all_zero] = len(self.classes_)\n",
      "\n",
      "                \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "                prob /= prob_sum.reshape((prob.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m))\n",
      "\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
      "\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError(\n",
      "                \u001b[33m\"predict_(log_)proba only supported when\"\u001b[39m\n",
      "                \u001b[33m\" loss='log_loss' or loss='modified_huber' \"\u001b[39m\n",
      "                \u001b[33m\"(%r given)\"\u001b[39m % self.loss\n",
      "            )\n",
      "\n",
      "    @available_if(_check_proba)\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m predict_log_proba(self, X):\n",
      "        \u001b[33m\"\"\"Log of probability estimates.\u001b[39m\n",
      "\n",
      "\u001b[33m        This method is only available for log loss and modified Huber loss.\u001b[39m\n",
      "\n",
      "\u001b[33m        When loss=\"modified_huber\", probability estimates may be hard zeros\u001b[39m\n",
      "\u001b[33m        and ones, so taking the logarithm is not possible.\u001b[39m\n",
      "\n",
      "\u001b[33m        See ``predict_proba`` for details.\u001b[39m\n",
      "\n",
      "\u001b[33m        Parameters\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39m\n",
      "\u001b[33m            Input data for prediction.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns\u001b[39m\n",
      "\u001b[33m        -------\u001b[39m\n",
      "\u001b[33m        T : array-like, shape (n_samples, n_classes)\u001b[39m\n",
      "\u001b[33m            Returns the log-probability of the sample for each class in the\u001b[39m\n",
      "\u001b[33m            model, where classes are ordered as they are in\u001b[39m\n",
      "\u001b[33m            `self.classes_`.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m np.log(self.predict_proba(X))\n",
      "\u001b[31mFile:\u001b[39m           ~/Documents/DiploDatos/IAA/IntroAprendizajeAutomatico/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "## valores de SGDClassifier por defecto\n",
    "SGDClassifier??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Pipeline completo (prepro + modelo)\n",
    "# ===============================\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento y predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Entrenamiento\n",
    "# ===============================\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# Predicci√≥n\n",
    "# ===============================\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Evaluaci√≥n en conjunto de ENTRENAMIENTO\n",
      "Accuracy: 0.8570465273095077\n",
      "Precision: 0.6099290780141844\n",
      "Recall: 0.35390946502057613\n",
      "F1 Score: 0.4479166666666667\n",
      "Matriz de confusi√≥n:\n",
      " [[1185   55]\n",
      " [ 157   86]]\n",
      "\n",
      "üîπ Evaluaci√≥n en conjunto de TEST\n",
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.6444444444444445\n",
      "Recall: 0.4393939393939394\n",
      "F1 Score: 0.5225225225225225\n",
      "Matriz de confusi√≥n:\n",
      " [[289  16]\n",
      " [ 37  29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ============\n",
    "# TRAIN\n",
    "# ============\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"üîπ Evaluaci√≥n en conjunto de ENTRENAMIENTO\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_train_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "# ============\n",
    "# TEST\n",
    "# ============\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"\\nüîπ Evaluaci√≥n en conjunto de TEST\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: proporci√≥n de predicciones correctas (puede ser enga√±oso en datasets con proporci√≥n de √©xito/fracaso desbalanceada)\n",
    "\n",
    "- Precision: de todos los positivos que predijo el modelo, ¬øcu√°ntos eran realmente positivos?\n",
    "\n",
    "- Recall: de todos los positivos reales, ¬øcu√°ntos detect√≥ el modelo?\n",
    "\n",
    "- F1-score: media arm√≥nica entre precision y recall.\n",
    "\n",
    "- Matriz de confusi√≥n: muestra TP, FP, TN, FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusi√≥n en contexto del problema\n",
    "- FP: Se predijo mora cuando el solicitante pag√≥ correctamente el cr√©dito. \n",
    "- FN: Se predijo cumplimiento cuando el solicitante incurri√≥ en mora. \n",
    "- TP: Se predijo mora y hubo mora. \n",
    "- TN: Se predijo cumplimiento y hubo cumplimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo presenta un valor de **accuracy** de 0.82. Aunque a primera vista parece alto, hay que tener presente que el dataset presenta un 20% de casos de mora (`target = 1`), por lo que un modelo que predijera `target = 0` para todos los casos tendr√≠a una accuracy de alrededor de 0.80. \n",
    "\n",
    "El valor de **precisi√≥n** es menor al 50% en las evaluaciones de ambos subconjuntos de datos. Esto significa que de los casos en que el modelo predijo que iba a haber mora, menos de la mitad lo fueron. Este no es un resultado alentador para la implementaci√≥n de este modelo ya que indica una alta proporci√≥n de falsos positivos. \n",
    "\n",
    "El valor de **recall**, menor al 30% en ambos subconjuntos de datos, indica que el modelo no est√° detectando m√°s del 70% de los casos reales de mora. \n",
    "\n",
    "EL F1-score es el indicador m√°s integral y confiable, sobre todo en un conjunto de datos con clases desbalanceadas. Valores tan bajos de F1-score reflejan un modelo con rendimiento pobre para detectar los casos de mora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusi√≥n\n",
    "\n",
    "El modelo es d√©bil para tareas sensibles como evaluaci√≥n crediticia. \n",
    "\n",
    "Se debe ajustar los hiperpar√°metros para mejorar la performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Ajuste de Hiperpar√°metros\n",
    "\n",
    "Seleccionar valores para los hiperpar√°metros principales del SGDClassifier. Como m√≠nimo, probar diferentes funciones de loss, tasas de entrenamiento y tasas de regularizaci√≥n.\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuraci√≥n encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci√≥n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi√≥n\n",
    "\n",
    "Documentaci√≥n:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c√≥digo explora:\n",
    "- 3 funciones de p√©rdida\n",
    "    - log-loss: regresi√≥n log√≠stica\n",
    "    - hinge: SVM lineal\n",
    "    - modified_huber: robusta a outliers\n",
    "- 3 valores de alpha (regularizaci√≥n)\n",
    "- 3 tipos de tasa de aprendizaje\n",
    "- 3 valores de tasa de aprendizaje inicial\n",
    "\n",
    "En total, se prueba 81 combinaciones de hiperpar√°metros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores combinaciones (top 5):\n",
      "                                                                                                                              params  mean_test_score  std_test_score\n",
      " {'classifier__alpha': 0.01, 'classifier__eta0': 0.01, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "  {'classifier__alpha': 0.01, 'classifier__eta0': 0.1, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "{'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "         {'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}         0.875259        0.010590\n",
      "           {'classifier__alpha': 0.01, 'classifier__eta0': 0.1, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}         0.875259        0.010590\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# ========= Preprocesamiento =========\n",
    "\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# ========= Definir hiperpar√°metros a probar =========\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__loss': ['log_loss', 'hinge', 'modified_huber'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],  # tasa de regularizaci√≥n\n",
    "    'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'classifier__eta0': [0.001, 0.01, 0.1]  # tasa de aprendizaje inicial\n",
    "}\n",
    "\n",
    "# ========= Configurar GridSearchCV =========\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# ========= Ejecutar b√∫squeda sobre X_train / y_train =========\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ========= Reportar resultados =========\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results_summary = results[['params', 'mean_test_score', 'std_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Mostrar las 10 mejores combinaciones\n",
    "print(\"Mejores combinaciones (top 5):\")\n",
    "print(results_summary.head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}\n"
     ]
    }
   ],
   "source": [
    "# imprimir hiperpar√°metros del mejor modelo\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El an√°lisis de validaci√≥n cruzada arroja que el **mejor modelo es una regresi√≥n log√≠stica con tasa de aprendizaje constante de 0.01 y un alpha de 0.001**, obteni√©ndose una accuracy promedio de 87%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el mejor modelo (pipeline completo ya entrenado con los mejores hiperpar√°metros)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluaci√≥n en conjunto de TEST (modelo final)\n",
      "Accuracy: 0.8544474393530997\n",
      "Precision: 0.8\n",
      "Recall: 0.24242424242424243\n",
      "F1 Score: 0.37209302325581395\n",
      "Matriz de confusi√≥n:\n",
      " [[301   4]\n",
      " [ 50  16]]\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones sobre test\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\nüìä Evaluaci√≥n en conjunto de TEST (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluaci√≥n en conjunto de ENTRENAMIENTO (modelo final)\n",
      "Accuracy: 0.8671611598111936\n",
      "Precision: 0.8709677419354839\n",
      "Recall: 0.2222222222222222\n",
      "F1 Score: 0.3540983606557377\n",
      "Matriz de confusi√≥n:\n",
      " [[1232    8]\n",
      " [ 189   54]]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "print(\"\\nüìä Evaluaci√≥n en conjunto de ENTRENAMIENTO (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_train_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretaci√≥n\n",
    "No hay mucha diferencia entre training y testing. No hay evidencias de overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Conclusi√≥n y perspectivas futuras\n",
    "Se trata de un modelo conservador. \n",
    "\n",
    "El modelo final es razonablemente bueno si el objetivo principal es minimizar falsos positivos (evitar rechazar a buenos clientes).\n",
    "\n",
    "Si el objetivo del negocio es detectar la mayor cantidad posible de morosos, este modelo necesita mejorar su recall.\n",
    "\n",
    "Una opci√≥n es bajar el umbral de decisi√≥n, lo cual mejorar√≠a la recall (mejor detecci√≥n de casos de mora) a costo de bajar la precisi√≥n (m√°s falsos positivos). La decisi√≥n de si seguir ese camino depende del tipo de error que sea prioritario minimizar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: √Årboles de Decisi√≥n\n",
    "\n",
    "En este ejercicio se entrenar√°n √°rboles de decisi√≥n para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deber√°n utilizar la clase DecisionTreeClassifier de scikit-learn.\n",
    "\n",
    "Documentaci√≥n:\n",
    "- https://scikit-learn.org/stable/modules/tree.html\n",
    "  - https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1: DecisionTreeClassifier con hiperpar√°metros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador DecisionTreeClassifier usando los valores por omisi√≥n de scikit-learn para todos los par√°metros. √önicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci√≥n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2: Ajuste de Hiperpar√°metros\n",
    "\n",
    "Seleccionar valores para los hiperpar√°metros principales del DecisionTreeClassifier. Como m√≠nimo, probar diferentes criterios de partici√≥n (criterion), profundidad m√°xima del √°rbol (max_depth), y cantidad m√≠nima de samples por hoja (min_samples_leaf).\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuraci√≥n encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci√≥n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi√≥n\n",
    "\n",
    "\n",
    "Documentaci√≥n:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
