{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Armado de un esquema de aprendizaje autom谩tico\n",
    "\n",
    "En el laboratorio final se espera que puedan poner en pr谩ctica los conocimientos adquiridos en el curso, trabajando con un conjunto de datos de clasificaci贸n.\n",
    "\n",
    "El objetivo es que se introduzcan en el desarrollo de un esquema para hacer tareas de aprendizaje autom谩tico: selecci贸n de un modelo, ajuste de hiperpar谩metros y evaluaci贸n.\n",
    "\n",
    "El conjunto de datos a utilizar est谩 en `./data/loan_data.csv`. Si abren el archivo ver谩n que al principio (las l铆neas que empiezan con `#`) describen el conjunto de datos y sus atributos (incluyendo el atributo de etiqueta o clase).\n",
    "\n",
    "Se espera que hagan uso de las herramientas vistas en el curso. Se espera que hagan uso especialmente de las herramientas brindadas por `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Agregar las librer铆as que hagan falta\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos y divisi贸n en entrenamiento y evaluaci贸n\n",
    "\n",
    "La celda siguiente se encarga de la carga de datos (haciendo uso de pandas). Estos ser谩n los que se trabajar谩n en el resto del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"./data/loan_data.csv\", comment=\"#\")\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/DiploDatos/IntroduccionAprendizajeAutomatico/master/data/loan_data.csv\", comment=\"#\")\n",
    "\n",
    "\n",
    "# Divisi贸n entre instancias y etiquetas\n",
    "X, y = dataset.iloc[:, 1:], dataset.TARGET\n",
    "\n",
    "# divisi贸n entre entrenamiento y evaluaci贸n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Documentaci贸n:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET     0\n",
      "LOAN       0\n",
      "MORTDUE    0\n",
      "VALUE      0\n",
      "YOJ        0\n",
      "DEROG      0\n",
      "DELINQ     0\n",
      "CLAGE      0\n",
      "NINQ       0\n",
      "CLNO       0\n",
      "DEBTINC    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# valores nulos\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que no hay valores nulos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci贸n original: TARGET\n",
      "0    0.833333\n",
      "1    0.166667\n",
      "Name: proportion, dtype: float64\n",
      "Train: TARGET\n",
      "0    0.836143\n",
      "1    0.163857\n",
      "Name: proportion, dtype: float64\n",
      "Test: TARGET\n",
      "0    0.822102\n",
      "1    0.177898\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribuci贸n de variable target en subconjuntos de train y test\n",
    "print(\"Distribuci贸n original:\", y.value_counts(normalize=True))\n",
    "print(\"Train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Test:\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que los subconjuntos de train y test presentan distribuciones semejantes de la variable TARGET. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Descripci贸n de los Datos y la Tarea\n",
    "\n",
    "Responder las siguientes preguntas:\n",
    "\n",
    "1. 驴De qu茅 se trata el conjunto de datos?\n",
    "\n",
    "The Home Equity dataset (HMEQ) contains baseline and loan performance\n",
    "information for 5,960 recent home equity loans.\n",
    "\n",
    "\n",
    "2. 驴Cu谩l es la variable objetivo que hay que predecir? 驴Qu茅 significado tiene?\n",
    "\n",
    "The target (BAD) is a binary\n",
    "variable indicating whether an applicant eventually defaulted or was\n",
    "seriously delinquent. \n",
    "This adverse outcome occurred in 1,189 cases (20%). \n",
    "\n",
    "3. 驴Qu茅 informaci贸n (atributos) hay disponible para hacer la predicci贸n?\n",
    "\n",
    "For\n",
    "each applicant, 12 input variables were recorded. \n",
    "\n",
    "LOAN    Amount of the loan request\n",
    "MORTDUE Amount due on existing mortgage\n",
    "VALUE   Value of current property\n",
    "YOJ     Years at present job\n",
    "DEROG   Number of major derogatory reports\n",
    "DELINQ  Number of delinquent credit lines\n",
    "CLAGE   Age of oldest trade line in months\n",
    "NINQ    Number of recent credit lines\n",
    "CLNO    Number of credit lines\n",
    "DEBTINC Debt-to-income ratio\n",
    "\n",
    "| Variable           | Tipo              | Descripci贸n                                                                                                                                                |\n",
    "| ------------------ | ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **TARGET** (o BAD) | Binaria (0/1)     | **Variable objetivo.** Indica si el solicitante cay贸 en default:<br> `0` = Pr茅stamo pagado correctamente.<br> `1` = Incumplimiento o morosidad severa. |\n",
    "| **LOAN**           | Num茅rica continua | Monto solicitado en el pr茅stamo.                                                                                                                    |\n",
    "| **MORTDUE**        | Num茅rica continua | Saldo actual de la hipoteca existente sobre la propiedad.                                                                                                  |\n",
    "| **VALUE**          | Num茅rica continua | Valor estimado de la propiedad del solicitante.                                                                                                            |\n",
    "| **YOJ**            | Num茅rica discreta | A帽os de antig眉edad del solicitante en su empleo actual.                                                                                                    |\n",
    "| **DEROG**          | Num茅rica discreta | Cantidad de reportes importantes de cr茅dito negativo (ej. bancarrotas, juicios).                                                                           |\n",
    "| **DELINQ**         | Num茅rica discreta | N煤mero de l铆neas de cr茅dito con moras registradas.                                                                                                         |\n",
    "| **CLAGE**          | Num茅rica continua | Antig眉edad de la l铆nea de cr茅dito m谩s antigua, en meses. Es un indicador de experiencia crediticia.                                                        |\n",
    "| **NINQ**           | Num茅rica discreta | N煤mero de l铆neas de cr茅dito abiertas recientemente (indicador de actividad reciente).                                                               |\n",
    "| **CLNO**           | Num茅rica discreta | N煤mero total de l铆neas de cr茅dito abiertas (tarjetas, pr茅stamos, etc.).                                                                                    |\n",
    "| **DEBTINC**        | Num茅rica continua | Relaci贸n deuda-ingresos (%). Se calcula como: *(total de deudas mensuales / ingreso mensual)*. Un valor alto puede indicar mayor riesgo crediticio.        |\n",
    "\n",
    "\n",
    "4. 驴Qu茅 atributos imagina ud. que son los m谩s determinantes para la predicci贸n?\n",
    "\n",
    "- **DEBTINC**: Es un indicador directo de la capacidad de pago del solicitante. Una persona con mucha deuda en relaci贸n a sus ingresos tiene m谩s riesgo de incumplir.\n",
    "\n",
    "Cuanto m谩s alto sea este ratio, m谩s probable es que haya dificultad para cumplir con nuevas obligaciones.\n",
    "\n",
    "- **DEROG**: La presencia de eventos crediticios graves (como bancarrota o juicios) es un fuerte indicador de riesgo hist贸rico.\n",
    "\n",
    "- **DELINQ**: Aunque menos grave que DEROG, refleja incumplimientos recientes o frecuentes, lo cual es 煤til para predecir problemas futuros.\n",
    "\n",
    "**No hace falta escribir c贸digo para responder estas preguntas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Predicci贸n con Modelos Lineales\n",
    "\n",
    "En este ejercicio se entrenar谩n modelos lineales de clasificaci贸n para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deber谩n utilizar la clase SGDClassifier de scikit-learn.\n",
    "\n",
    "Documentaci贸n:\n",
    "- https://scikit-learn.org/stable/modules/sgd.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos un escaleo de los datos ya que SGDClasiffier usa el m茅todo de descenso por gradiente, el cual es sensible a la escala. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las columnas son num茅ricas en este dataset\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1: SGDClassifier con hiperpar谩metros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador SGDClassifier usando los valores por omisi贸n de scikit-learn para todos los par谩metros. nicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci贸n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "SGDClassifier(\n",
      "    loss=\u001b[33m'hinge'\u001b[39m,\n",
      "    *,\n",
      "    penalty=\u001b[33m'l2'\u001b[39m,\n",
      "    alpha=\u001b[32m0.0001\u001b[39m,\n",
      "    l1_ratio=\u001b[32m0.15\u001b[39m,\n",
      "    fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    max_iter=\u001b[32m1000\u001b[39m,\n",
      "    tol=\u001b[32m0.001\u001b[39m,\n",
      "    shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    verbose=\u001b[32m0\u001b[39m,\n",
      "    epsilon=\u001b[32m0.1\u001b[39m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    learning_rate=\u001b[33m'optimal'\u001b[39m,\n",
      "    eta0=\u001b[32m0.0\u001b[39m,\n",
      "    power_t=\u001b[32m0.5\u001b[39m,\n",
      "    early_stopping=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    validation_fraction=\u001b[32m0.1\u001b[39m,\n",
      "    n_iter_no_change=\u001b[32m5\u001b[39m,\n",
      "    class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    average=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m SGDClassifier(BaseSGDClassifier):\n",
      "    \u001b[33m\"\"\"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\u001b[39m\n",
      "\n",
      "\u001b[33m    This estimator implements regularized linear models with stochastic\u001b[39m\n",
      "\u001b[33m    gradient descent (SGD) learning: the gradient of the loss is estimated\u001b[39m\n",
      "\u001b[33m    each sample at a time and the model is updated along the way with a\u001b[39m\n",
      "\u001b[33m    decreasing strength schedule (aka learning rate). SGD allows minibatch\u001b[39m\n",
      "\u001b[33m    (online/out-of-core) learning via the `partial_fit` method.\u001b[39m\n",
      "\u001b[33m    For best results using the default learning rate schedule, the data should\u001b[39m\n",
      "\u001b[33m    have zero mean and unit variance.\u001b[39m\n",
      "\n",
      "\u001b[33m    This implementation works with data represented as dense or sparse arrays\u001b[39m\n",
      "\u001b[33m    of floating point values for the features. The model it fits can be\u001b[39m\n",
      "\u001b[33m    controlled with the loss parameter; by default, it fits a linear support\u001b[39m\n",
      "\u001b[33m    vector machine (SVM).\u001b[39m\n",
      "\n",
      "\u001b[33m    The regularizer is a penalty added to the loss function that shrinks model\u001b[39m\n",
      "\u001b[33m    parameters towards the zero vector using either the squared euclidean norm\u001b[39m\n",
      "\u001b[33m    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\u001b[39m\n",
      "\u001b[33m    parameter update crosses the 0.0 value because of the regularizer, the\u001b[39m\n",
      "\u001b[33m    update is truncated to 0.0 to allow for learning sparse models and achieve\u001b[39m\n",
      "\u001b[33m    online feature selection.\u001b[39m\n",
      "\n",
      "\u001b[33m    Read more in the :ref:`User Guide <sgd>`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',\\\u001b[39m\n",
      "\u001b[33m        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',\\\u001b[39m\n",
      "\u001b[33m        'squared_epsilon_insensitive'}, default='hinge'\u001b[39m\n",
      "\u001b[33m        The loss function to be used.\u001b[39m\n",
      "\n",
      "\u001b[33m        - 'hinge' gives a linear SVM.\u001b[39m\n",
      "\u001b[33m        - 'log_loss' gives logistic regression, a probabilistic classifier.\u001b[39m\n",
      "\u001b[33m        - 'modified_huber' is another smooth loss that brings tolerance to\u001b[39m\n",
      "\u001b[33m          outliers as well as probability estimates.\u001b[39m\n",
      "\u001b[33m        - 'squared_hinge' is like hinge but is quadratically penalized.\u001b[39m\n",
      "\u001b[33m        - 'perceptron' is the linear loss used by the perceptron algorithm.\u001b[39m\n",
      "\u001b[33m        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\u001b[39m\n",
      "\u001b[33m          'squared_epsilon_insensitive' are designed for regression but can be useful\u001b[39m\n",
      "\u001b[33m          in classification as well; see\u001b[39m\n",
      "\u001b[33m          :class:`~sklearn.linear_model.SGDRegressor` for a description.\u001b[39m\n",
      "\n",
      "\u001b[33m        More details about the losses formulas can be found in the :ref:`User Guide\u001b[39m\n",
      "\u001b[33m        <sgd_mathematical_formulation>` and you can find a visualisation of the loss\u001b[39m\n",
      "\u001b[33m        functions in\u001b[39m\n",
      "\u001b[33m        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.\u001b[39m\n",
      "\n",
      "\u001b[33m    penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\u001b[39m\n",
      "\u001b[33m        The penalty (aka regularization term) to be used. Defaults to 'l2'\u001b[39m\n",
      "\u001b[33m        which is the standard regularizer for linear SVM models. 'l1' and\u001b[39m\n",
      "\u001b[33m        'elasticnet' might bring sparsity to the model (feature selection)\u001b[39m\n",
      "\u001b[33m        not achievable with 'l2'. No penalty is added when set to `None`.\u001b[39m\n",
      "\n",
      "\u001b[33m        You can see a visualisation of the penalties in\u001b[39m\n",
      "\u001b[33m        :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\u001b[39m\n",
      "\n",
      "\u001b[33m    alpha : float, default=0.0001\u001b[39m\n",
      "\u001b[33m        Constant that multiplies the regularization term. The higher the\u001b[39m\n",
      "\u001b[33m        value, the stronger the regularization. Also used to compute the\u001b[39m\n",
      "\u001b[33m        learning rate when `learning_rate` is set to 'optimal'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    l1_ratio : float, default=0.15\u001b[39m\n",
      "\u001b[33m        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\u001b[39m\n",
      "\u001b[33m        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\u001b[39m\n",
      "\u001b[33m        Only used if `penalty` is 'elasticnet'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, 1.0]` or can be `None` if\u001b[39m\n",
      "\u001b[33m        `penalty` is not `elasticnet`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 1.7\u001b[39m\n",
      "\u001b[33m            `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\u001b[39m\n",
      "\n",
      "\u001b[33m    fit_intercept : bool, default=True\u001b[39m\n",
      "\u001b[33m        Whether the intercept should be estimated or not. If False, the\u001b[39m\n",
      "\u001b[33m        data is assumed to be already centered.\u001b[39m\n",
      "\n",
      "\u001b[33m    max_iter : int, default=1000\u001b[39m\n",
      "\u001b[33m        The maximum number of passes over the training data (aka epochs).\u001b[39m\n",
      "\u001b[33m        It only impacts the behavior in the ``fit`` method, and not the\u001b[39m\n",
      "\u001b[33m        :meth:`partial_fit` method.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[1, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.19\u001b[39m\n",
      "\n",
      "\u001b[33m    tol : float or None, default=1e-3\u001b[39m\n",
      "\u001b[33m        The stopping criterion. If it is not None, training will stop\u001b[39m\n",
      "\u001b[33m        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\u001b[39m\n",
      "\u001b[33m        epochs.\u001b[39m\n",
      "\u001b[33m        Convergence is checked against the training loss or the\u001b[39m\n",
      "\u001b[33m        validation loss depending on the `early_stopping` parameter.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.19\u001b[39m\n",
      "\n",
      "\u001b[33m    shuffle : bool, default=True\u001b[39m\n",
      "\u001b[33m        Whether or not the training data should be shuffled after each epoch.\u001b[39m\n",
      "\n",
      "\u001b[33m    verbose : int, default=0\u001b[39m\n",
      "\u001b[33m        The verbosity level.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    epsilon : float, default=0.1\u001b[39m\n",
      "\u001b[33m        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\u001b[39m\n",
      "\u001b[33m        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\u001b[39m\n",
      "\u001b[33m        For 'huber', determines the threshold at which it becomes less\u001b[39m\n",
      "\u001b[33m        important to get the prediction exactly right.\u001b[39m\n",
      "\u001b[33m        For epsilon-insensitive, any differences between the current prediction\u001b[39m\n",
      "\u001b[33m        and the correct label are ignored if they are less than this threshold.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_jobs : int, default=None\u001b[39m\n",
      "\u001b[33m        The number of CPUs to use to do the OVA (One Versus All, for\u001b[39m\n",
      "\u001b[33m        multi-class problems) computation.\u001b[39m\n",
      "\u001b[33m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39m\n",
      "\u001b[33m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39m\n",
      "\u001b[33m        for more details.\u001b[39m\n",
      "\n",
      "\u001b[33m    random_state : int, RandomState instance, default=None\u001b[39m\n",
      "\u001b[33m        Used for shuffling the data, when ``shuffle`` is set to ``True``.\u001b[39m\n",
      "\u001b[33m        Pass an int for reproducible output across multiple function calls.\u001b[39m\n",
      "\u001b[33m        See :term:`Glossary <random_state>`.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[0, 2**32 - 1]`.\u001b[39m\n",
      "\n",
      "\u001b[33m    learning_rate : str, default='optimal'\u001b[39m\n",
      "\u001b[33m        The learning rate schedule:\u001b[39m\n",
      "\n",
      "\u001b[33m        - 'constant': `eta = eta0`\u001b[39m\n",
      "\u001b[33m        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\u001b[39m\n",
      "\u001b[33m          where `t0` is chosen by a heuristic proposed by Leon Bottou.\u001b[39m\n",
      "\u001b[33m        - 'invscaling': `eta = eta0 / pow(t, power_t)`\u001b[39m\n",
      "\u001b[33m        - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\u001b[39m\n",
      "\u001b[33m          Each time n_iter_no_change consecutive epochs fail to decrease the\u001b[39m\n",
      "\u001b[33m          training loss by tol or fail to increase validation score by tol if\u001b[39m\n",
      "\u001b[33m          `early_stopping` is `True`, the current learning rate is divided by 5.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'adaptive' option.\u001b[39m\n",
      "\n",
      "\u001b[33m    eta0 : float, default=0.0\u001b[39m\n",
      "\u001b[33m        The initial learning rate for the 'constant', 'invscaling' or\u001b[39m\n",
      "\u001b[33m        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\u001b[39m\n",
      "\u001b[33m        the default schedule 'optimal'.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `[0.0, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    power_t : float, default=0.5\u001b[39m\n",
      "\u001b[33m        The exponent for inverse scaling learning rate.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `(-inf, inf)`.\u001b[39m\n",
      "\n",
      "\u001b[33m    early_stopping : bool, default=False\u001b[39m\n",
      "\u001b[33m        Whether to use early stopping to terminate training when validation\u001b[39m\n",
      "\u001b[33m        score is not improving. If set to `True`, it will automatically set aside\u001b[39m\n",
      "\u001b[33m        a stratified fraction of training data as validation and terminate\u001b[39m\n",
      "\u001b[33m        training when validation score returned by the `score` method is not\u001b[39m\n",
      "\u001b[33m        improving by at least tol for n_iter_no_change consecutive epochs.\u001b[39m\n",
      "\n",
      "\u001b[33m        See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an\u001b[39m\n",
      "\u001b[33m        example of the effects of early stopping.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'early_stopping' option\u001b[39m\n",
      "\n",
      "\u001b[33m    validation_fraction : float, default=0.1\u001b[39m\n",
      "\u001b[33m        The proportion of training data to set aside as validation set for\u001b[39m\n",
      "\u001b[33m        early stopping. Must be between 0 and 1.\u001b[39m\n",
      "\u001b[33m        Only used if `early_stopping` is True.\u001b[39m\n",
      "\u001b[33m        Values must be in the range `(0.0, 1.0)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'validation_fraction' option\u001b[39m\n",
      "\n",
      "\u001b[33m    n_iter_no_change : int, default=5\u001b[39m\n",
      "\u001b[33m        Number of iterations with no improvement to wait before stopping\u001b[39m\n",
      "\u001b[33m        fitting.\u001b[39m\n",
      "\u001b[33m        Convergence is checked against the training loss or the\u001b[39m\n",
      "\u001b[33m        validation loss depending on the `early_stopping` parameter.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[1, max_iter)`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.20\u001b[39m\n",
      "\u001b[33m            Added 'n_iter_no_change' option\u001b[39m\n",
      "\n",
      "\u001b[33m    class_weight : dict, {class_label: weight} or \"balanced\", default=None\u001b[39m\n",
      "\u001b[33m        Preset for the class_weight fit parameter.\u001b[39m\n",
      "\n",
      "\u001b[33m        Weights associated with classes. If not given, all classes\u001b[39m\n",
      "\u001b[33m        are supposed to have weight one.\u001b[39m\n",
      "\n",
      "\u001b[33m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[39m\n",
      "\u001b[33m        weights inversely proportional to class frequencies in the input data\u001b[39m\n",
      "\u001b[33m        as ``n_samples / (n_classes * np.bincount(y))``.\u001b[39m\n",
      "\n",
      "\u001b[33m    warm_start : bool, default=False\u001b[39m\n",
      "\u001b[33m        When set to True, reuse the solution of the previous call to fit as\u001b[39m\n",
      "\u001b[33m        initialization, otherwise, just erase the previous solution.\u001b[39m\n",
      "\u001b[33m        See :term:`the Glossary <warm_start>`.\u001b[39m\n",
      "\n",
      "\u001b[33m        Repeatedly calling fit or partial_fit when warm_start is True can\u001b[39m\n",
      "\u001b[33m        result in a different solution than when calling fit a single time\u001b[39m\n",
      "\u001b[33m        because of the way the data is shuffled.\u001b[39m\n",
      "\u001b[33m        If a dynamic learning rate is used, the learning rate is adapted\u001b[39m\n",
      "\u001b[33m        depending on the number of samples already seen. Calling ``fit`` resets\u001b[39m\n",
      "\u001b[33m        this counter, while ``partial_fit`` will result in increasing the\u001b[39m\n",
      "\u001b[33m        existing counter.\u001b[39m\n",
      "\n",
      "\u001b[33m    average : bool or int, default=False\u001b[39m\n",
      "\u001b[33m        When set to `True`, computes the averaged SGD weights across all\u001b[39m\n",
      "\u001b[33m        updates and stores the result in the ``coef_`` attribute. If set to\u001b[39m\n",
      "\u001b[33m        an int greater than 1, averaging will begin once the total number of\u001b[39m\n",
      "\u001b[33m        samples seen reaches `average`. So ``average=10`` will begin\u001b[39m\n",
      "\u001b[33m        averaging after seeing 10 samples.\u001b[39m\n",
      "\u001b[33m        Integer values must be in the range `[1, n_samples]`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else \\\u001b[39m\n",
      "\u001b[33m            (n_classes, n_features)\u001b[39m\n",
      "\u001b[33m        Weights assigned to the features.\u001b[39m\n",
      "\n",
      "\u001b[33m    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\u001b[39m\n",
      "\u001b[33m        Constants in decision function.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_iter_ : int\u001b[39m\n",
      "\u001b[33m        The actual number of iterations before reaching the stopping criterion.\u001b[39m\n",
      "\u001b[33m        For multiclass fits, it is the maximum over every binary fit.\u001b[39m\n",
      "\n",
      "\u001b[33m    classes_ : array of shape (n_classes,)\u001b[39m\n",
      "\n",
      "\u001b[33m    t_ : int\u001b[39m\n",
      "\u001b[33m        Number of weight updates performed during training.\u001b[39m\n",
      "\u001b[33m        Same as ``(n_iter_ * n_samples + 1)``.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_features_in_ : int\u001b[39m\n",
      "\u001b[33m        Number of features seen during :term:`fit`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.24\u001b[39m\n",
      "\n",
      "\u001b[33m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39m\n",
      "\u001b[33m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[39m\n",
      "\u001b[33m        has feature names that are all strings.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.0\u001b[39m\n",
      "\n",
      "\u001b[33m    See Also\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    sklearn.svm.LinearSVC : Linear support vector classification.\u001b[39m\n",
      "\u001b[33m    LogisticRegression : Logistic regression.\u001b[39m\n",
      "\u001b[33m    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\u001b[39m\n",
      "\u001b[33m        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\u001b[39m\n",
      "\u001b[33m        penalty=None)``.\u001b[39m\n",
      "\n",
      "\u001b[33m    Examples\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    >>> import numpy as np\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.linear_model import SGDClassifier\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.preprocessing import StandardScaler\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.pipeline import make_pipeline\u001b[39m\n",
      "\u001b[33m    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\u001b[39m\n",
      "\u001b[33m    >>> Y = np.array([1, 1, 2, 2])\u001b[39m\n",
      "\u001b[33m    >>> # Always scale the input. The most convenient way is to use a pipeline.\u001b[39m\n",
      "\u001b[33m    >>> clf = make_pipeline(StandardScaler(),\u001b[39m\n",
      "\u001b[33m    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\u001b[39m\n",
      "\u001b[33m    >>> clf.fit(X, Y)\u001b[39m\n",
      "\u001b[33m    Pipeline(steps=[('standardscaler', StandardScaler()),\u001b[39m\n",
      "\u001b[33m                    ('sgdclassifier', SGDClassifier())])\u001b[39m\n",
      "\u001b[33m    >>> print(clf.predict([[-0.8, -1]]))\u001b[39m\n",
      "\u001b[33m    [1]\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    _parameter_constraints: dict = {\n",
      "        **BaseSGDClassifier._parameter_constraints,\n",
      "        \u001b[33m\"penalty\"\u001b[39m: [StrOptions({\u001b[33m\"l2\"\u001b[39m, \u001b[33m\"l1\"\u001b[39m, \u001b[33m\"elasticnet\"\u001b[39m}), \u001b[38;5;28;01mNone\u001b[39;00m],\n",
      "        \u001b[33m\"alpha\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "        \u001b[33m\"l1_ratio\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, closed=\u001b[33m\"both\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m],\n",
      "        \u001b[33m\"power_t\"\u001b[39m: [Interval(Real, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"neither\"\u001b[39m)],\n",
      "        \u001b[33m\"epsilon\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "        \u001b[33m\"learning_rate\"\u001b[39m: [\n",
      "            StrOptions({\u001b[33m\"constant\"\u001b[39m, \u001b[33m\"optimal\"\u001b[39m, \u001b[33m\"invscaling\"\u001b[39m, \u001b[33m\"adaptive\"\u001b[39m}),\n",
      "            Hidden(StrOptions({\u001b[33m\"pa1\"\u001b[39m, \u001b[33m\"pa2\"\u001b[39m})),\n",
      "        ],\n",
      "        \u001b[33m\"eta0\"\u001b[39m: [Interval(Real, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, closed=\u001b[33m\"left\"\u001b[39m)],\n",
      "    }\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        loss=\u001b[33m\"hinge\"\u001b[39m,\n",
      "        *,\n",
      "        penalty=\u001b[33m\"l2\"\u001b[39m,\n",
      "        alpha=\u001b[32m0.0001\u001b[39m,\n",
      "        l1_ratio=\u001b[32m0.15\u001b[39m,\n",
      "        fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        max_iter=\u001b[32m1000\u001b[39m,\n",
      "        tol=\u001b[32m1e-3\u001b[39m,\n",
      "        shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        verbose=\u001b[32m0\u001b[39m,\n",
      "        epsilon=DEFAULT_EPSILON,\n",
      "        n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        learning_rate=\u001b[33m\"optimal\"\u001b[39m,\n",
      "        eta0=\u001b[32m0.0\u001b[39m,\n",
      "        power_t=\u001b[32m0.5\u001b[39m,\n",
      "        early_stopping=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        validation_fraction=\u001b[32m0.1\u001b[39m,\n",
      "        n_iter_no_change=\u001b[32m5\u001b[39m,\n",
      "        class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        average=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            loss=loss,\n",
      "            penalty=penalty,\n",
      "            alpha=alpha,\n",
      "            l1_ratio=l1_ratio,\n",
      "            fit_intercept=fit_intercept,\n",
      "            max_iter=max_iter,\n",
      "            tol=tol,\n",
      "            shuffle=shuffle,\n",
      "            verbose=verbose,\n",
      "            epsilon=epsilon,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            learning_rate=learning_rate,\n",
      "            eta0=eta0,\n",
      "            power_t=power_t,\n",
      "            early_stopping=early_stopping,\n",
      "            validation_fraction=validation_fraction,\n",
      "            n_iter_no_change=n_iter_no_change,\n",
      "            class_weight=class_weight,\n",
      "            warm_start=warm_start,\n",
      "            average=average,\n",
      "        )\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _check_proba(self):\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.loss \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m (\u001b[33m\"log_loss\"\u001b[39m, \u001b[33m\"modified_huber\"\u001b[39m):\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m AttributeError(\n",
      "                \u001b[33m\"probability estimates are not available for loss=%r\"\u001b[39m % self.loss\n",
      "            )\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "    @available_if(_check_proba)\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m predict_proba(self, X):\n",
      "        \u001b[33m\"\"\"Probability estimates.\u001b[39m\n",
      "\n",
      "\u001b[33m        This method is only available for log loss and modified Huber loss.\u001b[39m\n",
      "\n",
      "\u001b[33m        Multiclass probability estimates are derived from binary (one-vs.-rest)\u001b[39m\n",
      "\u001b[33m        estimates by simple normalization, as recommended by Zadrozny and\u001b[39m\n",
      "\u001b[33m        Elkan.\u001b[39m\n",
      "\n",
      "\u001b[33m        Binary probability estimates for loss=\"modified_huber\" are given by\u001b[39m\n",
      "\u001b[33m        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\u001b[39m\n",
      "\u001b[33m        it is necessary to perform proper probability calibration by wrapping\u001b[39m\n",
      "\u001b[33m        the classifier with\u001b[39m\n",
      "\u001b[33m        :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\u001b[39m\n",
      "\n",
      "\u001b[33m        Parameters\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        X : {array-like, sparse matrix}, shape (n_samples, n_features)\u001b[39m\n",
      "\u001b[33m            Input data for prediction.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns\u001b[39m\n",
      "\u001b[33m        -------\u001b[39m\n",
      "\u001b[33m        ndarray of shape (n_samples, n_classes)\u001b[39m\n",
      "\u001b[33m            Returns the probability of the sample for each class in the model,\u001b[39m\n",
      "\u001b[33m            where classes are ordered as they are in `self.classes_`.\u001b[39m\n",
      "\n",
      "\u001b[33m        References\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        Zadrozny and Elkan, \"Transforming classifier scores into multiclass\u001b[39m\n",
      "\u001b[33m        probability estimates\", SIGKDD'02,\u001b[39m\n",
      "\u001b[33m        https://dl.acm.org/doi/pdf/10.1145/775047.775151\u001b[39m\n",
      "\n",
      "\u001b[33m        The justification for the formula in the loss=\"modified_huber\"\u001b[39m\n",
      "\u001b[33m        case is in the appendix B in:\u001b[39m\n",
      "\u001b[33m        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.loss == \u001b[33m\"log_loss\"\u001b[39m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._predict_proba_lr(X)\n",
      "\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m self.loss == \u001b[33m\"modified_huber\"\u001b[39m:\n",
      "            binary = len(self.classes_) == \u001b[32m2\u001b[39m\n",
      "            scores = self.decision_function(X)\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "                prob2 = np.ones((scores.shape[\u001b[32m0\u001b[39m], \u001b[32m2\u001b[39m))\n",
      "                prob = prob2[:, \u001b[32m1\u001b[39m]\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                prob = scores\n",
      "\n",
      "            np.clip(scores, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, prob)\n",
      "            prob += \u001b[32m1.0\u001b[39m\n",
      "            prob /= \u001b[32m2.0\u001b[39m\n",
      "\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "                prob2[:, \u001b[32m0\u001b[39m] -= prob\n",
      "                prob = prob2\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                \u001b[38;5;66;03m# the above might assign zero to all classes, which doesn't\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# normalize neatly; work around this to produce uniform\u001b[39;00m\n",
      "                \u001b[38;5;66;03m# probabilities\u001b[39;00m\n",
      "                prob_sum = prob.sum(axis=\u001b[32m1\u001b[39m)\n",
      "                all_zero = prob_sum == \u001b[32m0\u001b[39m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m np.any(all_zero):\n",
      "                    prob[all_zero, :] = \u001b[32m1\u001b[39m\n",
      "                    prob_sum[all_zero] = len(self.classes_)\n",
      "\n",
      "                \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "                prob /= prob_sum.reshape((prob.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m))\n",
      "\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
      "\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError(\n",
      "                \u001b[33m\"predict_(log_)proba only supported when\"\u001b[39m\n",
      "                \u001b[33m\" loss='log_loss' or loss='modified_huber' \"\u001b[39m\n",
      "                \u001b[33m\"(%r given)\"\u001b[39m % self.loss\n",
      "            )\n",
      "\n",
      "    @available_if(_check_proba)\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m predict_log_proba(self, X):\n",
      "        \u001b[33m\"\"\"Log of probability estimates.\u001b[39m\n",
      "\n",
      "\u001b[33m        This method is only available for log loss and modified Huber loss.\u001b[39m\n",
      "\n",
      "\u001b[33m        When loss=\"modified_huber\", probability estimates may be hard zeros\u001b[39m\n",
      "\u001b[33m        and ones, so taking the logarithm is not possible.\u001b[39m\n",
      "\n",
      "\u001b[33m        See ``predict_proba`` for details.\u001b[39m\n",
      "\n",
      "\u001b[33m        Parameters\u001b[39m\n",
      "\u001b[33m        ----------\u001b[39m\n",
      "\u001b[33m        X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39m\n",
      "\u001b[33m            Input data for prediction.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns\u001b[39m\n",
      "\u001b[33m        -------\u001b[39m\n",
      "\u001b[33m        T : array-like, shape (n_samples, n_classes)\u001b[39m\n",
      "\u001b[33m            Returns the log-probability of the sample for each class in the\u001b[39m\n",
      "\u001b[33m            model, where classes are ordered as they are in\u001b[39m\n",
      "\u001b[33m            `self.classes_`.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m np.log(self.predict_proba(X))\n",
      "\u001b[31mFile:\u001b[39m           ~/Documents/DiploDatos/IAA/IntroAprendizajeAutomatico/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "## valores de SGDClassifier por defecto\n",
    "SGDClassifier??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Pipeline completo (prepro + modelo)\n",
    "# ===============================\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento y predicci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Entrenamiento\n",
    "# ===============================\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# Predicci贸n\n",
    "# ===============================\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluaci贸n en conjunto de ENTRENAMIENTO\n",
      "Accuracy: 0.8570465273095077\n",
      "Precision: 0.6099290780141844\n",
      "Recall: 0.35390946502057613\n",
      "F1 Score: 0.4479166666666667\n",
      "Matriz de confusi贸n:\n",
      " [[1185   55]\n",
      " [ 157   86]]\n",
      "\n",
      " Evaluaci贸n en conjunto de TEST\n",
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.6444444444444445\n",
      "Recall: 0.4393939393939394\n",
      "F1 Score: 0.5225225225225225\n",
      "Matriz de confusi贸n:\n",
      " [[289  16]\n",
      " [ 37  29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ============\n",
    "# TRAIN\n",
    "# ============\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\" Evaluaci贸n en conjunto de ENTRENAMIENTO\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_train_pred))\n",
    "print(\"Matriz de confusi贸n:\\n\", confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "# ============\n",
    "# TEST\n",
    "# ============\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"\\n Evaluaci贸n en conjunto de TEST\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusi贸n:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: proporci贸n de predicciones correctas (puede ser enga帽oso en datasets con proporci贸n de 茅xito/fracaso desbalanceada)\n",
    "\n",
    "- Precision: de todos los positivos que predijo el modelo, 驴cu谩ntos eran realmente positivos?\n",
    "\n",
    "- Recall: de todos los positivos reales, 驴cu谩ntos detect贸 el modelo?\n",
    "\n",
    "- F1-score: media arm贸nica entre precision y recall.\n",
    "\n",
    "- Matriz de confusi贸n: muestra TP, FP, TN, FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusi贸n en contexto del problema\n",
    "- FP: Se predijo mora cuando el solicitante pag贸 correctamente el cr茅dito. \n",
    "- FN: Se predijo cumplimiento cuando el solicitante incurri贸 en mora. \n",
    "- TP: Se predijo mora y hubo mora. \n",
    "- TN: Se predijo cumplimiento y hubo cumplimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo presenta un valor de **accuracy** de 0.82. Aunque a primera vista parece alto, hay que tener presente que el dataset presenta un 20% de casos de mora (`target = 1`), por lo que un modelo que predijera `target = 0` para todos los casos tendr铆a una accuracy de alrededor de 0.80. \n",
    "\n",
    "El valor de **precisi贸n** es menor al 50% en las evaluaciones de ambos subconjuntos de datos. Esto significa que de los casos en que el modelo predijo que iba a haber mora, menos de la mitad lo fueron. Este no es un resultado alentador para la implementaci贸n de este modelo ya que indica una alta proporci贸n de falsos positivos. \n",
    "\n",
    "El valor de **recall**, menor al 30% en ambos subconjuntos de datos, indica que el modelo no est谩 detectando m谩s del 70% de los casos reales de mora. \n",
    "\n",
    "EL F1-score es el indicador m谩s integral y confiable, sobre todo en un conjunto de datos con clases desbalanceadas. Valores tan bajos de F1-score reflejan un modelo con rendimiento pobre para detectar los casos de mora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusi贸n\n",
    "\n",
    "El modelo es d茅bil para tareas sensibles como evaluaci贸n crediticia. \n",
    "\n",
    "Se debe ajustar los hiperpar谩metros para mejorar la performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Ajuste de Hiperpar谩metros\n",
    "\n",
    "Seleccionar valores para los hiperpar谩metros principales del SGDClassifier. Como m铆nimo, probar diferentes funciones de loss, tasas de entrenamiento y tasas de regularizaci贸n.\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuraci贸n encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci贸n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi贸n\n",
    "\n",
    "Documentaci贸n:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente c贸digo explora:\n",
    "- 3 funciones de p茅rdida\n",
    "    - log-loss: regresi贸n log铆stica\n",
    "    - hinge: SVM lineal\n",
    "    - modified_huber: robusta a outliers\n",
    "- 3 valores de alpha (regularizaci贸n)\n",
    "- 3 tipos de tasa de aprendizaje\n",
    "- 3 valores de tasa de aprendizaje inicial\n",
    "\n",
    "En total, se prueba 81 combinaciones de hiperpar谩metros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores combinaciones (top 5):\n",
      "                                                                                                                              params  mean_test_score  std_test_score\n",
      " {'classifier__alpha': 0.01, 'classifier__eta0': 0.01, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "  {'classifier__alpha': 0.01, 'classifier__eta0': 0.1, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "{'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}         0.875942        0.010999\n",
      "         {'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}         0.875259        0.010590\n",
      "           {'classifier__alpha': 0.01, 'classifier__eta0': 0.1, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}         0.875259        0.010590\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# ========= Preprocesamiento =========\n",
    "\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# ========= Definir hiperpar谩metros a probar =========\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__loss': ['log_loss', 'hinge', 'modified_huber'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],  # tasa de regularizaci贸n\n",
    "    'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'classifier__eta0': [0.001, 0.01, 0.1]  # tasa de aprendizaje inicial\n",
    "}\n",
    "\n",
    "# ========= Configurar GridSearchCV =========\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# ========= Ejecutar b煤squeda sobre X_train / y_train =========\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ========= Reportar resultados =========\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results_summary = results[['params', 'mean_test_score', 'std_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Mostrar las 10 mejores combinaciones\n",
    "print(\"Mejores combinaciones (top 5):\")\n",
    "print(results_summary.head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.01, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'modified_huber'}\n"
     ]
    }
   ],
   "source": [
    "# imprimir hiperpar谩metros del mejor modelo\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El an谩lisis de validaci贸n cruzada arroja que el **mejor modelo es una regresi贸n log铆stica con tasa de aprendizaje constante de 0.01 y un alpha de 0.001**, obteni茅ndose una accuracy promedio de 87%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el mejor modelo (pipeline completo ya entrenado con los mejores hiperpar谩metros)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluaci贸n en conjunto de TEST (modelo final)\n",
      "Accuracy: 0.8544474393530997\n",
      "Precision: 0.8\n",
      "Recall: 0.24242424242424243\n",
      "F1 Score: 0.37209302325581395\n",
      "Matriz de confusi贸n:\n",
      " [[301   4]\n",
      " [ 50  16]]\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones sobre test\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"\\n Evaluaci贸n en conjunto de TEST (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusi贸n:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluaci贸n en conjunto de ENTRENAMIENTO (modelo final)\n",
      "Accuracy: 0.8671611598111936\n",
      "Precision: 0.8709677419354839\n",
      "Recall: 0.2222222222222222\n",
      "F1 Score: 0.3540983606557377\n",
      "Matriz de confusi贸n:\n",
      " [[1232    8]\n",
      " [ 189   54]]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "print(\"\\n Evaluaci贸n en conjunto de ENTRENAMIENTO (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_train_pred))\n",
    "print(\"Matriz de confusi贸n:\\n\", confusion_matrix(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretaci贸n\n",
    "No hay mucha diferencia entre training y testing. No hay evidencias de overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Conclusi贸n y perspectivas futuras\n",
    "Se trata de un modelo conservador. \n",
    "\n",
    "El modelo final es razonablemente bueno si el objetivo principal es minimizar falsos positivos (evitar rechazar a buenos clientes).\n",
    "\n",
    "Si el objetivo del negocio es detectar la mayor cantidad posible de morosos, este modelo necesita mejorar su recall.\n",
    "\n",
    "Una opci贸n es bajar el umbral de decisi贸n, lo cual mejorar铆a la recall (mejor detecci贸n de casos de mora) a costo de bajar la precisi贸n (m谩s falsos positivos). La decisi贸n de si seguir ese camino depende del tipo de error que sea prioritario minimizar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: rboles de Decisi贸n\n",
    "\n",
    "En este ejercicio se entrenar谩n 谩rboles de decisi贸n para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deber谩n utilizar la clase DecisionTreeClassifier de scikit-learn.\n",
    "\n",
    "Documentaci贸n:\n",
    "- https://scikit-learn.org/stable/modules/tree.html\n",
    "  - https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1: DecisionTreeClassifier con hiperpar谩metros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador DecisionTreeClassifier usando los valores por omisi贸n de scikit-learn para todos los par谩metros. nicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci贸n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi贸n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2: Ajuste de Hiperpar谩metros\n",
    "\n",
    "Seleccionar valores para los hiperpar谩metros principales del DecisionTreeClassifier. Como m铆nimo, probar diferentes criterios de partici贸n (criterion), profundidad m谩xima del 谩rbol (max_depth), y cantidad m铆nima de samples por hoja (min_samples_leaf).\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuraci贸n encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluaci贸n**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusi贸n\n",
    "\n",
    "\n",
    "Documentaci贸n:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
